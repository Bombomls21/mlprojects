{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8CbYIl8EKH2d"
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YKIw4emJK1bu"
   },
   "source": [
    "**Machine Translation (MT)** aims to **automatically translate** text or speech from one **natural language** to another.\n",
    "It integrates concepts and techniques from **linguistics**, **computer science**, **probability and statistics**, and **artificial intelligence** to develop systems capable of producing accurate translations between human languages.\n",
    "\n",
    "Modern MT systems such as **Google Translate**, **Bing Translator**, and others have achieved **high-quality translations** and are now integrated into various platforms. These systems can translate effectively between **over 100 natural languages**.\n",
    "\n",
    "**Thus, the Input/Output of the MT problem is:**\n",
    "\n",
    "* **Input:** Source language text.\n",
    "\n",
    "  * Example (Vietnamese input): *\"Tôi đang học NLP\"*\n",
    "* **Output:** Translated text in the target language.\n",
    "\n",
    "  * Example (English translation): *\"I am learning NLP\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7CQITqhAK1gV"
   },
   "source": [
    "**Approaches to Machine Translation**:\n",
    "\n",
    "To effectively solve the **machine translation** problem, we need to focus on optimizing two key components:\n",
    "\n",
    "* **Part 1:** The learning algorithm to optimize the parameter set **θ**.\n",
    "* **Part 2:** The **decoding algorithm**, responsible for generating the best possible translation for the given input text.\n",
    "\n",
    "Currently, there are **three main approaches** to machine translation:\n",
    "\n",
    "1. **Rule-based Machine Translation (RBMT):** Translation based on linguistic rules.\n",
    "2. **Statistical Machine Translation (SMT):** Translation based on statistical models and probability.\n",
    "3. **Neural Machine Translation (NMT):** Translation using neural network architectures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0sG649R7MUve"
   },
   "source": [
    "**Focus of the Project**:\n",
    "\n",
    "Among these approaches, **Neural Machine Translation (NMT)** has shown **significant advancements** and produces **superior translation quality**.\n",
    "Therefore, this project focuses on **neural network–based methods**, consisting of two main parts:\n",
    "\n",
    "1. **Method 1:** Building a machine translation model using the **Transformer architecture**.\n",
    "2. **Method 2:** Building a machine translation model using **Pre-trained Language Models** such as **BERT** and **GPT**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dxcu-7jEMeRa"
   },
   "source": [
    "## Transformer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJpDmm4LM8uI"
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qrLCeH1BKojr"
   },
   "outputs": [],
   "source": [
    "# Import libs\n",
    "!pip install -q datasets sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A_XJjm2INTFC"
   },
   "outputs": [],
   "source": [
    "# download dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\n",
    "    \"mt_eng_vietnamese\",\n",
    "    \"iwslt2015-en-vi\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1709573372701,
     "user": {
      "displayName": "Duy Quang",
      "userId": "15026690002871949247"
     },
     "user_tz": -420
    },
    "id": "dqHSjuMoNTin",
    "outputId": "edba9236-9922-4389-87d2-5963157176b6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 133318\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 1269\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 1269\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1709573372701,
     "user": {
      "displayName": "Duy Quang",
      "userId": "15026690002871949247"
     },
     "user_tz": -420
    },
    "id": "lX884lwOPfM9",
    "outputId": "5ba5d6f3-66ac-40bc-b4c4-44500ca4e118"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['translation'],\n",
       "    num_rows: 133318\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 571,
     "status": "ok",
     "timestamp": 1709573373261,
     "user": {
      "displayName": "Duy Quang",
      "userId": "15026690002871949247"
     },
     "user_tz": -420
    },
    "id": "x1Hf3FsTOd4v",
    "outputId": "d565cccc-fc60-4f11-986d-e869a3641a65"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'Rachel Pike : The science behind a climate headline',\n",
       " 'vi': 'Khoa học đằng sau một tiêu đề về khí hậu'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train']['translation'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tk8Cnhp-Q1_p"
   },
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1709573373262,
     "user": {
      "displayName": "Duy Quang",
      "userId": "15026690002871949247"
     },
     "user_tz": -420
    },
    "id": "0VWzNOrhNTlE",
    "outputId": "23c1a8a9-3656-4570-b2a9-3a260bd7fe03"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': <function torchtext.data.utils._basic_english_normalize(line)>,\n",
       " 'vi': <function torchtext.data.utils._basic_english_normalize(line)>}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenization\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "SRC_LANGUAGE = 'en'\n",
    "TGT_LANGUAGE = 'vi'\n",
    "\n",
    "token_transform = {} # tokenizer\n",
    "vocab_transform = {} # vocab\n",
    "\n",
    "token_transform[SRC_LANGUAGE] = get_tokenizer('basic_english')\n",
    "token_transform[TGT_LANGUAGE] = get_tokenizer('basic_english')\n",
    "token_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8396,
     "status": "ok",
     "timestamp": 1709573381646,
     "user": {
      "displayName": "Duy Quang",
      "userId": "15026690002871949247"
     },
     "user_tz": -420
    },
    "id": "ifsPQ9nENTnh",
    "outputId": "b9c87590-ffd8-4c43-ee6d-289229be339a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en vocab length: 29114\n",
      "vi vocab length: 12099\n"
     ]
    }
   ],
   "source": [
    "# Building vocabulary\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "def yield_tokens(data_iter, language):\n",
    "    for data_sample in data_iter['translation']:\n",
    "        yield token_transform[language](data_sample[language])\n",
    "\n",
    "for language in [SRC_LANGUAGE, TGT_LANGUAGE]: # en, vi\n",
    "    train_iter = data['train']\n",
    "\n",
    "    vocab_transform[language] = build_vocab_from_iterator(\n",
    "        yield_tokens(train_iter, language),\n",
    "        min_freq=2,\n",
    "        specials=special_symbols,\n",
    "        special_first=True\n",
    "    )\n",
    "    vocab_transform[language].set_default_index(UNK_IDX)\n",
    "    print(f'{language} vocab length: {len(vocab_transform[language].get_stoi())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1709573381647,
     "user": {
      "displayName": "Duy Quang",
      "userId": "15026690002871949247"
     },
     "user_tz": -420
    },
    "id": "m_YgbP-oSoyz",
    "outputId": "b3f76ff7-f34f-45c3-945f-7616f5c4db02"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', '<pad>', '<bos>', '<eos>', ',', '.', 'the', 'and', 'to', '&apos']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transform[SRC_LANGUAGE].get_itos()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1709573381647,
     "user": {
      "displayName": "Duy Quang",
      "userId": "15026690002871949247"
     },
     "user_tz": -420
    },
    "id": "WOS6qCaCSsum",
    "outputId": "d1e0e803-c80d-4dab-da19-509a64554f68"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', '<pad>', '<bos>', '<eos>', ',', '.', 'và', 'tôi', 'là', 'một']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transform[TGT_LANGUAGE].get_itos()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1709573381647,
     "user": {
      "displayName": "Duy Quang",
      "userId": "15026690002871949247"
     },
     "user_tz": -420
    },
    "id": "HBm6x3djS3Bu",
    "outputId": "fd963904-4b02-4b21-81cb-a016a90a0f9a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29114, 12099)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_transform[SRC_LANGUAGE]), len(vocab_transform[TGT_LANGUAGE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3AsdoAAtVaM1"
   },
   "outputs": [],
   "source": [
    "# def lowercase(text):\n",
    "#   return text.lower()\n",
    "\n",
    "# def remove_punctuation(text):\n",
    "#   import string\n",
    "#   return ''.join([char for char in text if char not in string.punctuation])\n",
    "\n",
    "# # Combine the transformations\n",
    "# text_transform = sequential_transforms(lowercase, remove_punctuation)\n",
    "\n",
    "# # Example usage\n",
    "# original_text = \"Hello, World! How are you?\"\n",
    "# transformed_text = text_transform(original_text)\n",
    "\n",
    "# print(original_text)  # Output: Hello, World! How are you?\n",
    "# print(transformed_text) # Output: hello world how are you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1709573381647,
     "user": {
      "displayName": "Duy Quang",
      "userId": "15026690002871949247"
     },
     "user_tz": -420
    },
    "id": "jisZAmQhXG0_",
    "outputId": "df631afd-72c2-478e-8f9d-12bba556190d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6429, 17576, 6, 295, 553, 11, 682, 5334]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transform['en'](['rachel', 'pike', 'the', 'science', 'behind', 'a', 'climate', 'headline'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A516TAUbTUsF"
   },
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1709573381647,
     "user": {
      "displayName": "Duy Quang",
      "userId": "15026690002871949247"
     },
     "user_tz": -420
    },
    "id": "HQVrwjx7NTuU",
    "outputId": "49ca6d5a-6a67-4778-a0ee-284139309be4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function sequential_transforms.<locals>.func at 0x792c9ed365f0>\n",
      "<function sequential_transforms.<locals>.func at 0x792c9ed36680>\n"
     ]
    }
   ],
   "source": [
    "# Dataloader\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms): # This part uses the asterisk (*) operator to allow accepting a variable number of transformation functions as arguments.\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "\n",
    "    return func\n",
    "\n",
    "# function to add BOS /EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids):\n",
    "    return torch.cat((torch.tensor([BOS_IDX]),\n",
    "                      torch.tensor(token_ids),\n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# ‘‘src‘‘ and ‘‘tgt‘' language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {} # token --> indices\n",
    "\n",
    "for language in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    text_transform[language] = sequential_transforms(\n",
    "        token_transform[language], # Tokenization\n",
    "        vocab_transform[language], # Numericalization\n",
    "        tensor_transform # Add BOS /EOS and create tensor\n",
    "    )\n",
    "    print(text_transform[language])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0u4zoOfNQVbK"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# function to collate data samples into batch tensors\n",
    "def collate_fn(batch):\n",
    "    # batch_size = 2\n",
    "    # [{'en': 'Rachel Pike : The science behind a climate headline', 'vi': 'Khoa học đằng sau một tiêu đề về khí hậu'},\n",
    "    # {'en': 'In 4 minutes , atmospheric chemist Rachel Pike provides a glimpse of the massive scientific effort behind the bold headlines on climate change , with her team -- one of thousands who contributed -- taking a risky flight over the rainforest in pursuit of data on a key molecule .', 'vi': 'Trong 4 phút , chuyên gia hoá học khí quyển Rachel Pike giới thiệu sơ lược về những nỗ lực khoa học miệt mài đằng sau những tiêu đề táo bạo về biến đổi khí hậu , cùng với đoàn nghiên cứu của mình -- hàng ngàn người đã cống hiến cho dự án này -- một chuyến bay mạo hiểm qua rừng già để tìm kiếm thông tin về một phân tử then chốt .'}]\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for sample in batch:\n",
    "        src_sample, tgt_sample = sample[SRC_LANGUAGE], sample[TGT_LANGUAGE]\n",
    "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample).to(dtype=torch.int64))\n",
    "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample).to(dtype=torch.int64))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "    return src_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jay1QClgTi4J"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "        data['train']['translation'],\n",
    "        batch_size=BATCH_SIZE ,\n",
    "        collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader (\n",
    "        data['validation']['translation'],\n",
    "        batch_size=BATCH_SIZE,\n",
    "        collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader (\n",
    "        data['test']['translation'],\n",
    "        batch_size=BATCH_SIZE,\n",
    "        collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1709573382361,
     "user": {
      "displayName": "Duy Quang",
      "userId": "15026690002871949247"
     },
     "user_tz": -420
    },
    "id": "aQa1pN7VM0Zi",
    "outputId": "15f55bb3-a20a-4359-dd99-ba4ebb8bce43"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 52]), torch.Size([8, 78]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = next(iter(train_dataloader))\n",
    "data[0].shape, data[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q8bmQcwgM_wc"
   },
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VXT5o2VaQTcz"
   },
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "import math\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gp2lDoI1QJbH"
   },
   "outputs": [],
   "source": [
    "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "\n",
    "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size) # Scales the embeddings (optional)\n",
    "\n",
    "\n",
    "# Seq2Seq Network\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "\n",
    "        self.transformer = Transformer(d_model=emb_size,\n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout,\n",
    "                                       batch_first=True)\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                src_mask: Tensor,\n",
    "                tgt_mask: Tensor,\n",
    "                src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor):\n",
    "\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src)) # [8, 52, 512]\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg)) # [8, 77, 512]\n",
    "\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
    "                                src_padding_mask, tgt_padding_mask)\n",
    "        return self.generator(outs)\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        return self.transformer.encoder(self.positional_encoding(self.src_tok_emb(src)), src_mask)\n",
    "\n",
    "    def decode(self, tgt: Tensor, context: Tensor, tgt_mask: Tensor):\n",
    "        return self.transformer.decoder(self.positional_encoding(self.tgt_tok_emb(tgt)), context, tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "calXbs6LKe7Z"
   },
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "    '''\n",
    "    return:\n",
    "        src_mask: mask của encoder input\n",
    "        tgt_mask: mask của decoder input\n",
    "        src_padding_mask: mask cho token không phải padding của src\n",
    "        tgt_padding_mask: mask cho token không phải padding của tgt\n",
    "    '''\n",
    "    src_seq_len = src.shape[1]\n",
    "    tgt_seq_len = tgt.shape[1]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vBjuMf2RHRQv"
   },
   "outputs": [],
   "source": [
    "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE]) # 29114\n",
    "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE]) # 12099\n",
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "BATCH_SIZE = 128\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE, NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44,
     "status": "ok",
     "timestamp": 1709573383034,
     "user": {
      "displayName": "Duy Quang",
      "userId": "15026690002871949247"
     },
     "user_tz": -420
    },
    "id": "XnM_suDEWuEH",
    "outputId": "a58b3d4a-65b6-421c-f14f-fd79d72c399d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12099"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TGT_VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1709573383034,
     "user": {
      "displayName": "Duy Quang",
      "userId": "15026690002871949247"
     },
     "user_tz": -420
    },
    "id": "boADlOz0Vqvc",
    "outputId": "628666eb-3a3b-434d-b9d1-1ea830b30f87"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "src_ids, tgt_ids = next(iter(train_dataloader))\n",
    "src_ids = src_ids.to(DEVICE)\n",
    "tgt_ids = tgt_ids.to(DEVICE)\n",
    "\n",
    "tgt_input = tgt_ids[:, :-1]\n",
    "tgt_output = tgt_ids[:, 1:]\n",
    "\n",
    "src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src_ids, tgt_input)\n",
    "logits = transformer(src_ids, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_output.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1709573383034,
     "user": {
      "displayName": "Duy Quang",
      "userId": "15026690002871949247"
     },
     "user_tz": -420
    },
    "id": "hZK60xkIXsAi",
    "outputId": "c57dd4e0-96cd-4bf6-c8ec-6bc60aabafaf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False, False, False, False, False, False, False,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True], device='cuda:0')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_padding_mask[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1709573383034,
     "user": {
      "displayName": "Duy Quang",
      "userId": "15026690002871949247"
     },
     "user_tz": -420
    },
    "id": "fnUk5swbZA8v",
    "outputId": "31fa28f3-10a5-42bc-9c18-fda4322eb2af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 52])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1709573383035,
     "user": {
      "displayName": "Duy Quang",
      "userId": "15026690002871949247"
     },
     "user_tz": -420
    },
    "id": "5k1IUzpNrem7",
    "outputId": "48c9af09-ef00-4633-8918-d1977e42af09"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 77])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1709573383035,
     "user": {
      "displayName": "Duy Quang",
      "userId": "15026690002871949247"
     },
     "user_tz": -420
    },
    "id": "A9AuFVwcra4Q",
    "outputId": "3b493f02-01c3-4586-8a14-88c98e251b52"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 77, 12099])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 605,
     "status": "ok",
     "timestamp": 1709573383634,
     "user": {
      "displayName": "Duy Quang",
      "userId": "15026690002871949247"
     },
     "user_tz": -420
    },
    "id": "kSWhl-F5rTth",
    "outputId": "09fb6220-f836-40c9-d35e-8184f5a59127"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.5250, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9vrX84xrlNg"
   },
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ev9xtNWBrnVg"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train_epoch(model, optimizer, criterion, train_dataloader, device):\n",
    "    model.train()\n",
    "    losses = []\n",
    "\n",
    "    for src_ids, tgt_ids in train_dataloader:\n",
    "        src_ids = src_ids.to(device)\n",
    "        tgt_ids = tgt_ids.to(device)\n",
    "\n",
    "        tgt_input = tgt_ids[:, :-1]\n",
    "        tgt_output = tgt_ids[:, 1:]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src_ids, tgt_input)\n",
    "        try:\n",
    "            output = model(\n",
    "                src_ids, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask\n",
    "            )\n",
    "        except:\n",
    "            print(src_ids.shape, tgt_input.shape)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = criterion(\n",
    "            output.reshape(-1, output.shape[-1]),\n",
    "            tgt_output.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    return sum(losses) / len(losses)\n",
    "\n",
    "def evaluate(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for src_ids, tgt_ids in data_loader:\n",
    "            src_ids = src_ids.to(device)\n",
    "            tgt_ids = tgt_ids.to(device)\n",
    "\n",
    "            tgt_input = tgt_ids[:, :-1]\n",
    "            tgt_output = tgt_ids[:, 1:]\n",
    "\n",
    "            src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src_ids, tgt_input)\n",
    "            output = model(\n",
    "                src_ids, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask\n",
    "            )\n",
    "            loss = criterion(\n",
    "                output.reshape(-1, output.shape[-1]),\n",
    "                tgt_output.reshape(-1)\n",
    "            )\n",
    "            losses.append(loss.item())\n",
    "    return sum(losses) / len(losses)\n",
    "\n",
    "def train(model, train_dataloader, valid_dataloader, optimizer, criterion, device, epochs):\n",
    "    for epoch in range(1, epochs+1):\n",
    "        start_time = time.time()\n",
    "        train_loss = train_epoch(model, optimizer, criterion, train_dataloader, device)\n",
    "        valid_loss = evaluate(model, valid_dataloader, criterion, device)\n",
    "        end_time = time.time()\n",
    "        print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {valid_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "szlnDCZwr-V2"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2004967,
     "status": "ok",
     "timestamp": 1709576572814,
     "user": {
      "displayName": "Duy Quang",
      "userId": "15026690002871949247"
     },
     "user_tz": -420
    },
    "id": "214f3947-e3e9-4455-be61-2b9b5f34a2f6",
    "outputId": "0cbded74-06c0-455e-d737-c73284c313a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 4.585, Val loss: 4.135, Epoch time = 646.748s\n",
      "Epoch: 2, Train loss: 3.957, Val loss: 3.824, Epoch time = 641.112s\n",
      "Epoch: 3, Train loss: 3.712, Val loss: 3.669, Epoch time = 636.455s\n",
      "Epoch: 4, Train loss: 3.554, Val loss: 3.550, Epoch time = 631.623s\n",
      "Epoch: 5, Train loss: 3.442, Val loss: 3.477, Epoch time = 632.205s\n"
     ]
    }
   ],
   "source": [
    "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
    "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "BATCH_SIZE = 128\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE, NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "epochs = 5\n",
    "train(transformer, train_dataloader, valid_dataloader, optimizer, criterion, DEVICE, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OiwCV152sroH"
   },
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1709578274926,
     "user": {
      "displayName": "Duy Quang",
      "userId": "15026690002871949247"
     },
     "user_tz": -420
    },
    "id": "UfiYtdKzwmcN",
    "outputId": "e1743f6d-65e0-471c-8c5a-33c1ce43cd88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.8702, 0.9839, 0.0636],\n",
      "         [0.5654, 0.8888, 0.7106]]])\n",
      "torch.Size([1, 2, 3])\n",
      "-----------------------------------\n",
      "tensor([[[0.8702, 0.9839, 0.0636]],\n",
      "\n",
      "        [[0.5654, 0.8888, 0.7106]]])\n",
      "torch.Size([2, 1, 3])\n",
      "-----------------------------------\n",
      "tensor([[0.8702, 0.9839, 0.0636],\n",
      "        [0.5654, 0.8888, 0.7106]])\n",
      "torch.Size([2, 3])\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(1, 2, 3)\n",
    "print(tensor)\n",
    "print(tensor.shape)\n",
    "print('-'*35)\n",
    "out = tensor.transpose(0, 1)\n",
    "print(out)\n",
    "print(out.shape)\n",
    "print('-'*35)\n",
    "out = out[:, -1]\n",
    "print(out)\n",
    "print(out.shape)\n",
    "print('-'*35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wDX_UVNrsu_q"
   },
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    src = src.to(DEVICE)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "\n",
    "    # Encode the source sequence using the model encoder\n",
    "    context = model.encode(src, src_mask)\n",
    "\n",
    "    # Create a starting token (usually an end-of-sentence symbol)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    for i in range(max_len-1):\n",
    "        context = context.to(DEVICE)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(1)).type(torch.bool)).to(DEVICE)\n",
    "\n",
    "        # Decode based on the current target sequence and encoded source representation\n",
    "        out = model.decode(ys, context, tgt_mask) # [1, 1, 512], [1, 2, 512], [1, 3, 512], [1, 4, 512], [1, 5, 512], [1, 6, 512]\n",
    "        out = out.transpose(0, 1)                 # [1, 1, 512], [2, 1, 512], [3, 1, 512], [4, 1, 512], [5, 1, 512], [6, 1, 512]\n",
    "        prob = model.generator(out[:, -1])        # [1, 12099],  [2, 12099],  [3, 12099],  [4, 12099],  [5, 12099],  [6, 12099]\n",
    "        # transpose chiều câu lên ví trí số 2 rồi out[:, -1] để xóa nó đi\n",
    "\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word[-1].item()\n",
    "\n",
    "        # [[2, 7]], [[ 2,  7, 78]], [[ 2,  7, 78, 66]], [[ 2,  7, 78, 66, 156]], [[ 2,  7, 78, 66, 156, 5]], [[ 2,  7, 78, 66, 156, 5, 3]]\n",
    "        ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
    "\n",
    "        if next_word == EOS_IDX:\n",
    "            break\n",
    "    return ys\n",
    "\n",
    "\n",
    "# actual function to translate input sentence into target language\n",
    "def translate(model: torch.nn.Module, src_sentence: str):\n",
    "    model.eval()\n",
    "    src = text_transform[SRC_LANGUAGE](src_sentence).view(1, -1)\n",
    "    num_tokens = src.shape[1]\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    tgt_tokens = greedy_decode(model, src, src_mask, max_len=num_tokens+5, start_symbol=BOS_IDX).flatten()\n",
    "\n",
    "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 557
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1709576974922,
     "user": {
      "displayName": "Duy Quang",
      "userId": "15026690002871949247"
     },
     "user_tz": -420
    },
    "id": "16fNjLjGt_sd",
    "outputId": "ac1147d2-b5fd-4417-f70d-2dbf41a26d7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 512])\n",
      "torch.Size([1, 1, 512])\n",
      "torch.Size([1, 12099])\n",
      "tensor([[2, 7]], device='cuda:0')\n",
      "\n",
      "torch.Size([1, 2, 512])\n",
      "torch.Size([2, 1, 512])\n",
      "torch.Size([2, 12099])\n",
      "tensor([[ 2,  7, 78]], device='cuda:0')\n",
      "\n",
      "torch.Size([1, 3, 512])\n",
      "torch.Size([3, 1, 512])\n",
      "torch.Size([3, 12099])\n",
      "tensor([[ 2,  7, 78, 66]], device='cuda:0')\n",
      "\n",
      "torch.Size([1, 4, 512])\n",
      "torch.Size([4, 1, 512])\n",
      "torch.Size([4, 12099])\n",
      "tensor([[  2,   7,  78,  66, 156]], device='cuda:0')\n",
      "\n",
      "torch.Size([1, 5, 512])\n",
      "torch.Size([5, 1, 512])\n",
      "torch.Size([5, 12099])\n",
      "tensor([[  2,   7,  78,  66, 156,   5]], device='cuda:0')\n",
      "\n",
      "torch.Size([1, 6, 512])\n",
      "torch.Size([6, 1, 512])\n",
      "torch.Size([6, 12099])\n",
      "tensor([[  2,   7,  78,  66, 156,   5,   3]], device='cuda:0')\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' tôi đi học trường . '"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(transformer, \"i go to school\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1709576572827,
     "user": {
      "displayName": "Duy Quang",
      "userId": "15026690002871949247"
     },
     "user_tz": -420
    },
    "id": "Psr4ldCuv33f",
    "outputId": "5705213b-cfb6-4372-e67e-5bec9d84aa12"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' tôi đi học trường . '"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(transformer, \"i go to school\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1709576572835,
     "user": {
      "displayName": "Duy Quang",
      "userId": "15026690002871949247"
     },
     "user_tz": -420
    },
    "id": "22ZvMBf6v5U6",
    "outputId": "cd8f77ef-4416-4d2a-8ff6-68fd1351c2ad"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' bạn đang ở đây hôm nay ? '"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(transformer, \"How are you today?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1709576572842,
     "user": {
      "displayName": "Duy Quang",
      "userId": "15026690002871949247"
     },
     "user_tz": -420
    },
    "id": "jBjZgzI2v5Ys",
    "outputId": "9866a6b1-644f-4ccd-babd-64c94c2aae9b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' <unk> <unk> '"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(transformer, \"wassup dawg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 136441,
     "status": "ok",
     "timestamp": 1709576903859,
     "user": {
      "displayName": "Duy Quang",
      "userId": "15026690002871949247"
     },
     "user_tz": -420
    },
    "id": "u1U943sHv38E",
    "outputId": "db3d4da1-1425-4626-c332-a460d53f3288"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1269/1269 [02:15<00:00,  9.35it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BLEU = 7.12 44.0/16.1/6.1/2.3 (BP = 0.717 ratio = 0.751 hyp_len = 25322 ref_len = 33738)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import sacrebleu\n",
    "\n",
    "pred_sentences, tgt_sentences = [], []\n",
    "for sample in tqdm(data['test']['translation']):\n",
    "    src_sentence = sample[SRC_LANGUAGE]\n",
    "    tgt_sentence = sample[TGT_LANGUAGE]\n",
    "\n",
    "    pred_sentence = translate(transformer, src_sentence)\n",
    "    pred_sentences.append(pred_sentence)\n",
    "\n",
    "    tgt_sentences.append(tgt_sentence)\n",
    "\n",
    "bleu_score = sacrebleu.corpus_bleu(pred_sentences, [tgt_sentences], force=True)\n",
    "bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aSR8SVOr1MKY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPcQKHP1Ox97KYBE7dKromk",
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
